# ğŸ§  GPT-2 Fine-Tuning on Custom Educational Data

This project fine-tunes OpenAI's GPT-2 model on a custom dataset of university-level educational content, including Generative AI, Machine Learning, Deep Learning, and expository writing. The result is a model that generates coherent and formal text in an academic style.

---

## ğŸ“š Project Description

This internship project is focused on training and fine-tuning GPT-2 using Hugging Face's Transformers library. A `.txt` dataset containing formal academic content is used to teach the model how to generate contextually relevant and structured educational text.

---

## ğŸš€ Features

- Fine-tunes GPT-2 using Hugging Face Transformers
- Accepts plain `.txt` dataset input
- Trains with Google Colab + GPU
- Generates formal, coherent university-style content
- Tracks metrics with optional Weights & Biases integration

---

## ğŸ› ï¸ Installation & Setup

1. Clone the repo or open in Google Colab.
2. Install dependencies:

```bash
!pip install transformers datasets
!pip install wandb
